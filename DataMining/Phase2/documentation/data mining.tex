\documentclass{sig-alternate}

\begin{document}
	\title{CSCI-620 Data Mining with the Airbnb Dataset}
	\subtitle{[Exploring and Mining the dataset of NY Airbnbs]}
	
	\numberofauthors{4}
	\author
	{
		\alignauthor
		Aishwarya Rao
		\email{ar2711@rit.edu}
		\alignauthor
		Apurav Khare
		\email{ak2816@rit.edu}
		\and
		\alignauthor
		Martin Qian
		\email{jq3513@rit.edu}
		\alignauthor
		Prateek Kalasannavar
		\email{pk6685@rit.edu}
	}
	
	\maketitle
	\begin{abstract}
		
	
	\end{abstract}
	
	\section{Trials on the dataset}
	\subsection{Decision Tree}
	A decision Tree is is a decision support tool that uses a tree-like model of decisions and their possible consequences, 
	including chance event outcomes, resource costs, and utility. 
	

	Because the data has many attributes of discrete, in order to do decision tree on our dataset, 
	At first we did some simple modifications to the data. These modifications will be applied to 
	the other 2 algorithms, too. What we did first was to remove all the extreme data, such as the data with prices
	that are 0 meaning these hosts are totally free or that are too expensive that are higher than 1000 dollars. These
	data is generated in abnormal conditions and will greatly impact the result of decision tree model.
	Secondly, we kept all the attributes of numberics and discarded other attributes. This step might looks like controversial
	yet as we are doing just test, we think it is okay. Moreover, our aim is to make predictions yet decision tree
	can only do classification, so we converted the aim attribute price from numberics to boolean value. The means of the
	dataset is about 130, so that every price greater than 100 will be tagged as true, otherwise false.
	Last but not least, we divided the dataset to train and test by 9:1. 

	The final result is about 72\% and concrete report can be found here

	\subsection{k nearest neighbours}
	k nearest neighbours(kNN) is a simple yet powerful algorithm for data mining. In k-NN classification, 
	the output is a class membership. An object is classified by a plurality vote of its neighbors, 
	with the object being assigned to the class most common among its k nearest neighbors. For example, if k = 1, 
	then the object is simply assigned to the class of that single nearest neighbor.
	In addition, the data used by kNN needs to be normalized because it uses distance to calculate varieties. 
	What's more, k of kNN is a hyper-parameter, and there's no best k actually, but a popular choice of k is between 5-20.

	After doing the same processing, the final result of kNN is 64\% with k=20. Here is the report.

	\subsection{Naive Bayes}
	Naive Bayes is another simple classifier based on applying Bayes' 
	theorem with strong (naïve) independence assumptions between the features. 
	Naive Bayes is better resistance to noise compared to the former 2, however, we get rid of most
	noise in the pre-processing. 
	
	The result of Naive Bayes is 68\%. Here is the report.

	\subsection{summaries}
	After discussion, we decided to use decision trees as our algorithm to draw final conclusions.
	Here is also a picture showing how these algorithms' accuracy along with different devision.
	(x-col should be multipled by 50 and blue is decision tree, yellow is naïve Bayes, red is kNN)
    \figure{line-diagram.jpg}

	
\end{document}